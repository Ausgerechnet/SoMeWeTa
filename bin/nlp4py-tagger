#!/usr/bin/env python3

import argparse
import collections
import functools
import gzip
import itertools
import json
import logging
import multiprocessing
import operator
import os
import random
import re
import statistics

from nlp4py import utils

logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)


def arguments():
    """Process command line arguments."""
    parser = argparse.ArgumentParser(description="An averaged perceptron part-of-speech tagger")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--train", type=os.path.abspath, help="Train the tagger on the input corpus and write the model to the specified file")
    group.add_argument("--tag", type=os.path.abspath, help="Tag the input corpus using the specified model")
    group.add_argument("--evaluate", type=os.path.abspath, help="Evaluate the performance of the specified model on the input corpus")
    group.add_argument("--crossvalidate", action="store_true", help="Evaluate tagger performance via 10-fold cross-validation on the input corpus")
    parser.add_argument("--brown", type=argparse.FileType("r"), help="""Brown clusters (paths output file
                        produced by wcluster (https://github.com/percyliang/brown-cluster); optional and only for
                        training or cross-validation""")
    parser.add_argument("--w2v", type=argparse.FileType("r"), help="Word2Vec vectors; optional and only for training or cross-validation")
    parser.add_argument("--lexicon", type=os.path.abspath, help="Additional full-form lexicon; optional and only for training or cross-validation")
    parser.add_argument("--prior", type=os.path.abspath, help="Prior weights, i.e. a model trained on another corpus; optional and only for training or cross-validation")
    parser.add_argument("-i", "--iterations", type=int, default=10, help="Only for training or cross-validation: Number of iterations; default: 10")
    parser.add_argument("-b", "--beam-size", type=int, default=5, help="Size of the search beam; default: 5")
    parser.add_argument("CORPUS", type=argparse.FileType("r"),
                        help="""Input corpus. Path to a file or "-" for STDIN. Format for training,
                             evaluation and cross-validation: One
                             token-pos pair per line, separated by a
                             tab; sentences delimited by an empty
                             line. Format for tagging: One token per
                             line; sentences delimited by an empty
                             line.""")
    return parser.parse_args()


Beam = collections.namedtuple("Beam", ["tags", "weight_sum", "features"])


def train_by_iterative_parameter_mixing(training_data, iterations=10, beam_size=5, n_shards=5):
    """Iterative parameter mixing was first described by McDonald et al.
    (2010).

    """
    weights = collections.defaultdict(lambda: collections.defaultdict(float))
    weights_c = collections.defaultdict(lambda: collections.defaultdict(float))
    counter = 0
    random.seed(42)
    random.shuffle(training_data)
    div, mod = divmod(len(training_data), n_shards)
    shards = [training_data[i * div + min(i, mod):(i + 1) * div + min(i + 1, mod)] for i in range(n_shards)]
    for it in range(iterations):
        ipm = functools.partial(ipm_iteration, it=it, beam_size=beam_size, w=weights, wc=weights_c, c=counter)
        results = map(ipm, shards)
        weights = collections.defaultdict(lambda: collections.defaultdict(float))
        weights_c = collections.defaultdict(lambda: collections.defaultdict(float))
        counter, correct, total, early_update = 0, 0, 0, 0
        for w, wc, c, cor, tot, earl_upd in results:
            for feat in w:
                for cls, weight in w[feat].items():
                    weights[feat][cls] += weight / n_shards
            for feat in wc:
                for cls, weight in wc[feat].items():
                        weights_c[feat][cls] += weight / n_shards
            counter += c / n_shards
            correct += cor
            total += tot
            early_update += earl_upd
        logging.info("Iteration %d: %d/%d = %.2f%% (%d early update)" % (it, correct, total, (correct / total) * 100, early_update))
    # return weights - weights_c / counter
    for feat in weights:
        for cls in weights[feat]:
            weights[feat][cls] -= weights_c[feat][cls] / counter
    return weights


def ipm_iteration(training_data, it, beam_size, w, wc, c):
    """"""
    weights = collections.defaultdict(lambda: collections.defaultdict(float))
    weights_c = collections.defaultdict(lambda: collections.defaultdict(float))
    counter = c
    random.seed(it)
    random.shuffle(training_data)
    for feat in w:
        for cls, weight in w[feat].items():
            weights[feat][cls] = weight
    for feat in wc:
        for cls, weight in wc[feat].items():
            weights_c[feat][cls] = weight
    total, incorrect, early_update = 0, 0, 0
    for structure in training_data:
        items, static_features, gold_classes = structure
        predicted, features = beam_search(items, static_features, weights, beam_size, gold_classes)
        assert type(predicted) == type(gold_classes)
        if len(predicted) != len(gold_classes):
            early_update += 1
        if predicted != gold_classes:
            incorrect += 1
            weights, weights_c = update(weights, weights_c, gold_classes, predicted, features, counter)
        counter += len(predicted)
        total += 1
    correct = total - incorrect
    return weights, weights_c, counter, correct, total, early_update


def train(words, static_features, tags, lengths, prior_weights=None, iterations=10, beam_size=5):
    """An implementation of the averaged structured perceptron learning
    algorithm.

    training_data is a tuple of (items, features, classes) pairs. items
    is a tuple, features is a tuple of sets, classes is a tuple.

    """
    counter = 0
    weights = collections.defaultdict(lambda: collections.defaultdict(float))
    weights_c = collections.defaultdict(lambda: collections.defaultdict(float))
    sentence_ranges = list(zip((a - b for a, b in zip(itertools.accumulate(lengths), lengths)), lengths))
    for it in range(iterations):
        total, incorrect, early_update = 0, 0, 0
        for start, length in sentence_ranges:
            items = words[start:start + length]
            stat_feats = static_features[start:start + length]
            gold_classes = tags[start:start + length]
            predicted, features = beam_search(items, stat_feats, weights, prior_weights, beam_size, gold_classes)
            assert type(predicted) == type(gold_classes)
            if len(predicted) != len(gold_classes):
                early_update += 1
            if predicted != gold_classes:
                incorrect += 1
                weights, weights_c = update(weights, weights_c, gold_classes, predicted, features, counter)
            # counter += 1
            counter += len(predicted)
            total += 1
        random.seed(it)
        random.shuffle(sentence_ranges)
        correct = total - incorrect
        logging.info("Iteration %d: %d/%d = %.2f%% (%d early update)" % (it, correct, total, (correct / total) * 100, early_update))
    # return weights - weights_c / counter
    for feat in weights:
        for cls in weights[feat]:
            weights[feat][cls] -= weights_c[feat][cls] / counter
    if prior_weights is not None:
        for feat in prior_weights:
            for cls, weight in prior_weights[feat].items():
                weights[feat][cls] += weight
    return weights


def tag(words, static_features, lengths, weights, beam_size=5):
    """"""
    start = 0
    for length in lengths:
        items = words[start:start + length]
        stat_feats = static_features[start:start + length]
        start += length
        predicted, features = beam_search(items, stat_feats, weights, prior_weights=None, beam_size=beam_size)
        yield zip(items, predicted)


def beam_search(words, static_features, weights, prior_weights, beam_size, gold_classes=None):
    """"""
    words = [w.lower() for w in words]
    beams = [Beam([], 0, [])]
    gold_tags = []
    for i, word in enumerate(words):
        agenda = []
        weight_sum = predict_static(static_features[i], weights, prior_weights)
        for beam in beams:
            latent_features = get_latent_features(words, beam.tags, i)
            features = static_features[i] | latent_features
            for prediction, weight in predict_latent(latent_features, weights, prior_weights, weight_sum, beam_size):
                agenda.append(Beam(beam.tags + [prediction], beam.weight_sum + weight, beam.features + [features]))
        beams = sorted(agenda, key=operator.attrgetter("weight_sum"), reverse=True)[:beam_size]
        if gold_classes is not None:
            gold_tags.append(gold_classes[i])
            beam_tags = [beam.tags for beam in beams]
            if all(bt != gold_tags for bt in beam_tags):
                break
    tags, weights, features = beams[0]
    return tags, features


def predict_static(features, weights, prior_weights):
    """"""
    weight_sum = collections.defaultdict(float)
    for feat in features:
        if feat in weights:
            for cls, weight in weights[feat].items():
                weight_sum[cls] += weight
        if prior_weights is not None and feat in prior_weights:
            for cls, weight in prior_weights[feat].items():
                weight_sum[cls] += weight
    return weight_sum


def predict_latent(features, weights, prior_weights, static_weights, beam_size):
    """"""
    weight_sum = collections.defaultdict(float)
    for cls, weight in static_weights.items():
        weight_sum[cls] = weight
    for feat in features:
        if feat in weights:
            for cls, weight in weights[feat].items():
                weight_sum[cls] += weight
        if prior_weights is not None and feat in prior_weights:
            for cls, weight in prior_weights[feat].items():
                weight_sum[cls] += weight
    if len(weight_sum) > 0:
        # Add class labels to break ties
        return [_ for _ in sorted(weight_sum.items(), key=operator.itemgetter(1, 0), reverse=True)][:beam_size]
    else:
        return [(None, 0)]


def predict(features, weights, prior_weights):
    """"""
    weight_sum = collections.defaultdict(float)
    # feature_weights = [weights.get(feat) for feat in features]
    # feature_weights = [fw.items() for fw in feature_weights if fw is not None]
    # for cls, weight in itertools.chain.from_iterable(feature_weights):
    #     weight_sum[cls] += weight
    # if prior_weights is not None:
    #     feature_weights = [prior_weights.get(feat) for feat in features]
    #     feature_weights = [fw.items() for fw in feature_weights if fw is not None]
    #     for cls, weight in itertools.chain.from_iterable(feature_weights):
    #         weight_sum[cls] += weight
    for feat in features:
        if feat in weights:
            for cls, weight in weights[feat].items():
                weight_sum[cls] += weight
        if prior_weights is not None and feat in prior_weights:
            for cls, weight in prior_weights[feat].items():
                weight_sum[cls] += weight
    if len(weight_sum) > 0:
        # Add class labels to break ties
        return [_ for _ in sorted(weight_sum.items(), key=operator.itemgetter(1, 0), reverse=True)]
    else:
        return [(None, 0)]


def update(weights, weights_c, gold, predicted, features, counter):
    """"""
    for feature_set, gold_cls, predicted_cls in zip(features, gold, predicted):
        if gold_cls != predicted_cls:
            for feat in feature_set:
                weights[feat][gold_cls] += 1
                weights_c[feat][gold_cls] += counter
                if predicted_cls is not None:
                    weights[feat][predicted_cls] -= 1
                    weights_c[feat][predicted_cls] -= counter
            # for feat in feature_set:
            #     try:
            #         weights[feat][gold_cls] += 1
            #     except KeyError:
            #         if feat not in weights:
            #             weights[feat] = dict()
            #         if gold_cls not in weights[feat]:
            #             weights[feat][gold_cls] = 1
            #     weights_c[feat][gold_cls] += counter
            #     if predicted_cls is not None:
            #         try:
            #             weights[feat][predicted_cls] -= 1
            #         except KeyError:
            #             if predicted_cls not in weights[feat]:
            #                 weights[feat][predicted_cls] = -1
            #         weights_c[feat][predicted_cls] -= counter
        counter += 1
    return weights, weights_c


def save(model, filename):
    """"""
    with gzip.open(filename, 'wb') as f:
        f.write(json.dumps(model, ensure_ascii=False, indent=4).encode())


def load(filename):
    """"""
    with gzip.open(filename, 'rb') as f:
        model = json.loads(f.read().decode())
    return model


def get_latent_features(words, beam, i):
    """"""
    features = set()
    tags = ["<START-2>", "<START-1>"] + beam
    j = i + 2
    if i >= 1:
        features.add("P1_word, P1_pos: %s, %s" % (words[i - 1], tags[j - 1]))
    if i >= 2:
        features.add("P2_word, P2_pos: %s, %s" % (words[i - 2], tags[j - 2]))
    features.add("P1_pos: %s" % tags[j - 1])
    features.add("P2_pos: %s" % tags[j - 2])
    features.add("P2_pos, P1_pos: %s, %s" % (tags[j - 2], tags[j - 1]))
    features.add("P1_pos, W_word: %s, %s" % (tags[j - 1], words[i]))
    return features


def word_shape(word):
    if len(word) >= 100:
        return "LONG"
    shape = []
    last = ""
    shape_char = ""
    seq = 0
    for c in word:
        if c.isalpha():
            if c.isupper():
                shape_char = "X"
            else:
                shape_char = "x"
        elif c.isdigit():
            shape_char = "d"
        else:
            shape_char = c
        if shape_char == last:
            seq += 1
        else:
            seq = 0
            last = shape_char
        if seq < 4:
            shape.append(shape_char)
    return "".join(shape)


def word_flags(word, prefix):
    """"""
    email = re.compile(r"^[[:alnum:].%+-]+(?:@| \[?at\]? )[[:alnum:].-]+(?:\.| \[?dot\]? )[[:alpha:]]{2,}$", re.IGNORECASE)
    tag = re.compile(r"^</?[^>]+>$")
    url = re.compile(r"^(?:(?:(?:https?|ftp|svn)://|(?:https?://)?www\.).+)|(?:[\w./-]+\.(?:de|com|org|net|edu|info|jpg|png|gif|log|txt)(?:-\w+)?)$", re.IGNORECASE)
    mention = re.compile(r"^@\w+$")
    hashtag = re.compile(r"^#\w+$")
    action_word = re.compile(r"^[*+][^*]+[*]$")
    punctuation = re.compile(r'^[](){}.!?…<>%‰€$£₤¥°@~*„“”‚‘"\'`´»«›‹,;:/*+=&%§~#^−–-]+$')
    ordinal = re.compile(r"^(?:\d+\.)+$")
    number = re.compile(r"""(?<!\w)
                            (?:[−+-]?              # optional sign
                              \d*                  # optional digits before decimal point
                              [.,]?                # optional decimal point
                              \d+                  # digits
                              (?:[eE][−+-]?\d+)?   # optional exponent
                              |
                              \d+[\d.,]*\d+)
                            (?![.,]?\d)""", re.VERBOSE)
    emoticon_set = set(["(-.-)", "(T_T)", "(♥_♥)", ")':", ")-:",
                        "(-:", ")=", ")o:", ")x", ":'C", ":/", ":<",
                        ":C", ":[", "=(", "=)", "=D", "=P", ">:",
                        "D':", "D:", "\:", "]:", "x(", "^^", "o.O",
                        "oO", "\O/", "\m/", ":;))", "_))", "*_*",
                        "._.", ":wink:", ">_<", "*<:-)", ":!:",
                        ":;-))"])
    emoticon_list = sorted(emoticon_set, key=len, reverse=True)
    emoticon = re.compile(r"""^(?:(?:[:;]|(?<!\d)8)           # a variety of eyes, alt.: [:;8]
                                [-'oO]?                       # optional nose or tear
                                (?: \)+ | \(+ | [*] | ([DPp])\1*(?!\w)))   # a variety of mouths
                           """ +
                          r"|" +
                          r"(?:xD+|XD+)" +
                          r"|" +
                          r"([:;])[ ]+([()])" +
                          r"|" +
                          r"\^3"
                          r"|" +
                          r"emojiQ[[:alpha:]][3,}"
                          r"|" +
                          r"[\u2600-\u27BF\U0001F300-\U0001f64f\U0001F680-\U0001F6FF\U0001F900-\U0001F9FF]" +  # Unicode emoticons and other symbols
                          r"|" +
                          r"|".join([re.escape(_) for _ in emoticon_list]) +
                          r"$", re.VERBOSE)
    flags = set()
    if word.isalpha():
        flags.add("%s_isalpha" % prefix)
    if word.isnumeric():
        flags.add("%s_isnumeric" % prefix)
    if word.islower():
        flags.add("%s_islower" % prefix)
    if word.isupper():
        flags.add("%s_isupper" % prefix)
    if word.istitle():
        flags.add("%s_istitle" % prefix)
    if email.search(word):
        flags.add("%s_isemail" % prefix)
    if tag.search(word):
        flags.add("%s_istag" % prefix)
    if url.search(word):
        flags.add("%s_isurl" % prefix)
    if mention.search(word):
        flags.add("%s_ismention" % prefix)
    if hashtag.search(word):
        flags.add("%s_ishashtag" % prefix)
    if action_word.search(word):
        flags.add("%s_isactword" % prefix)
    if emoticon.search(word):
        flags.add("%s_isemoticon" % prefix)
    if punctuation.search(word):
        flags.add("%s_ispunct" % prefix)
    if ordinal.search(word):
        flags.add("%s_isordinal" % prefix)
    if number.search(word):
        flags.add("%s_isnumber" % prefix)
    return flags


def get_static_features(words, lengths, lexicon=None, brown_clusters=None, word_to_vec=None):
    """"""
    features = []
    start = 0
    for length in lengths:
        sentence = words[start:start + length]
        start += length
        tokens = ["<START-2>", "<START-1>"] + [w.lower() for w in sentence] + ["<END+1>", "<END+2>"]
        for i, word in enumerate(sentence):
            j = i + 2
            local_features = set()
            w = tokens[j]
            p1 = tokens[j - 1]
            p2 = tokens[j - 2]
            n1 = tokens[j + 1]
            n2 = tokens[j + 2]
            # constant bias feature acts like a prior
            local_features.add("bias")
            # word length
            # local_features.add("W_length: %d" % len(word))
            # current word
            local_features.add("W_word: %s" % w)
            # next words
            local_features.add("N1_word: %s" % n1)
            local_features.add("N2_word: %s" % n2)
            # affixes
            local_features.add("W_prefix: %s" % w[:3])
            local_features.add("W_suffix: %s" % w[-3:])
            if i >= 1:
                local_features.add("P1_suffix: %s" % p1[-3:])
            if length - i > 1:
                local_features.add("N1_suffix: %s" % n1[-3:])
            # word shape
            local_features.add("W_shape: %s" % word_shape(word))
            # Flags
            if i >= 2:
                local_features.update(word_flags(p2, "P2"))
            if i >= 1:
                local_features.update(word_flags(p1, "P1"))
            local_features.update(word_flags(w, "W"))
            if length - i > 1:
                local_features.update(word_flags(n1, "N1"))
            if length - i > 2:
                local_features.update(word_flags(n2, "N2"))
            # Brown clusters
            if brown_clusters is not None:
                # P2, P1, W, N1, N2
                if i >= 2 and p2 in brown_clusters:
                    local_features.add("P2_brown: %s" % brown_clusters[p2])
                if i >= 1 and p1 in brown_clusters:
                    local_features.add("P1_brown: %s" % brown_clusters[p1])
                if w in brown_clusters:
                    local_features.add("W_brown: %s" % brown_clusters[w])
                if length - i > 1 and n1 in brown_clusters:
                    local_features.add("N1_brown: %s" % brown_clusters[n1])
                if length - i > 2 and n2 in brown_clusters:
                    local_features.add("N2_brown: %s" % brown_clusters[n2])
            if word_to_vec is not None:
                # if w in word_to_vec:
                #     for i, d in enumerate(word_to_vec[w]):
                #         local_features.add("W_w2v_%d: %d" % (i, round(float(d))))
                if w in word_to_vec:
                    local_features.add("W_w2v: %s" % word_to_vec[w])
            if lexicon is not None:
                if w in lexicon:
                    for feat in lexicon[w]:
                        local_features.add("W_lex: %s" % feat)
            features.append(local_features)
    return features


def evaluate(gold_corpus, tagged_corpus):
    """Evaluate accuracy of `tagged_corpus` against `gold_corpus`."""
    words, gold_tags = zip(*gold_corpus)
    predicted_tags = [p for s in tagged_corpus for w, p in s]
    # words, predicted_tags = zip(itertools.chain.from_iterable(tagged_corpus))
    total = len(gold_tags)
    correct = sum(1 for gt, pt in zip(gold_tags, predicted_tags) if gt == pt)
    return correct / total


def cross_val_iteration(i, words, features, tags, lengths, sentence_ranges, prior_weights, div, mod, iterations, beam_size):
    """"""
    test_ranges = sentence_ranges[i * div + min(i, mod):(i + 1) * div + min(i + 1, mod)]
    test_start = test_ranges[0][0]
    test_end = test_ranges[-1][0] + test_ranges[-1][1]
    test_lengths = lengths[i * div + min(i, mod):(i + 1) * div + min(i + 1, mod)]
    train_lengths = lengths[:i * div + min(i, mod)] + lengths[(i + 1) * div + min(i + 1, mod):]
    test_words = words[test_start:test_end]
    test_features = features[test_start:test_end]
    test_tags = tags[test_start:test_end]
    gold_corpus = zip(test_words, test_tags)
    train_words = words[:test_start] + words[test_end:]
    train_features = features[:test_start] + features[test_end:]
    train_tags = tags[:test_start] + tags[test_end:]
    model = train(train_words, train_features, train_tags, train_lengths, prior_weights, iterations, beam_size)
    tagged_corpus = tag(test_words, test_features, test_lengths, model, beam_size)
    return evaluate(gold_corpus, tagged_corpus) * 100


def main():
    args = arguments()
    lexicon, brown_clusters, word_to_vec, prior_weights = None, None, None, None
    if args.lexicon:
        lexicon = utils.read_lexicon(args.lexicon)
    if args.brown:
        brown_clusters = utils.read_brown_clusters(args.brown)
    if args.w2v:
        word_to_vec = utils.read_word2vec_vectors(args.w2v)
    if args.prior:
        prior_weights = load(args.prior)
    if args.train:
        words, tags, lengths = utils.read_corpus(args.CORPUS, tagged=True)
        features = get_static_features(words, lengths, lexicon, brown_clusters, word_to_vec)
        model = train(words, features, tags, lengths, prior_weights, args.iterations, args.beam_size)
        save(model, args.train)
    elif args.tag:
        model = load(args.tag)
        words, lengths = utils.read_corpus(args.CORPUS, tagged=False)
        features = get_static_features(words, lengths, lexicon, brown_clusters, word_to_vec)
        tagged_corpus = tag(words, features, lengths, model, args.beam_size)
        for sentence in tagged_corpus:
            print("\n".join(["\t".join(t) for t in sentence]), "\n")
    elif args.evaluate:
        model = load(args.evaluate)
        words, tags, lengths = utils.read_corpus(args.CORPUS)
        gold_corpus = zip(words, tags)
        features = get_static_features(words, lengths, lexicon, brown_clusters, word_to_vec)
        tagged_corpus = tag(words, features, lengths, model, args.beam_size)
        print("Accuracy: %.2f" % (evaluate(gold_corpus, tagged_corpus) * 100,))
    elif args.crossvalidate:
        words, tags, lengths = utils.read_corpus(args.CORPUS)
        sentence_ranges = list(zip((a - b for a, b in zip(itertools.accumulate(lengths), lengths)), lengths))
        features = get_static_features(words, lengths, lexicon, brown_clusters, word_to_vec)
        div, mod = divmod(len(sentence_ranges), 10)
        cvi = functools.partial(cross_val_iteration, words=words,
                                features=features, tags=tags, lengths=lengths,
                                sentence_ranges=sentence_ranges, prior_weights=prior_weights,
                                div=div, mod=mod, iterations=args.iterations,
                                beam_size=args.beam_size)
        with multiprocessing.Pool() as pool:
            accuracies = pool.map(cvi, range(10))
            # accuracies = list(map(cvi, range(10)))
        print("accuracy: %.2f ±%.2f" % (statistics.mean(accuracies), 2 * statistics.stdev(accuracies)))


if __name__ == "__main__":
    main()
